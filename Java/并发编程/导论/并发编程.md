# Introduction

很多时候我们看的高并发系统设计或者分布式系统设计这些文章中，都会提到锁、事务、原子操作等等概念。以原子操作为例，我们可能会碰到讲 AtomicInteger 的文章，也可能会碰到讨论 ACID 或者分布式事务的问题，这三者目标都是为了实现原子操作或者事务，但是它们问题的来源以及解决方案还是有一定区别，因此笔者在自己的知识体系内将这些分别放置到并发编程领域与分布式系统设计领域进行阐述。
并发编程与分布式系统中都存在着所谓的同步问题，而同步问题基本的就是解决原子性与可见性/一致性这两个问题，其基本手段就是基于锁，因此又可以分为三个方面：指令串行化/临界资源管理/锁、数据一致性/数据可见性、事务/原子操作。其区别在于并发编程中这种问题的产生是因为内存中的高速缓存与指令重排/代码重排，其主要解决方式是内存屏障。- 锁：分布式系统中倾向于利用 Redis/Zookeeper 这些第三方进程构建一个全局锁，而并发编程中着眼于在内存模型中构建一个全局锁。- 数据一致性：并发编程中着眼解决高速缓存带来的数据可见性问题，而分布式系统中着眼于不同数据存储之间的同步。关于所谓的线性一致性、顺序一致性、最终一致性等讨论会放到分布式系统中。- 原子操作/事务：一般来说，原子操作与事务都是基于锁实现的，自然因为不同构建锁的方式也把不同的原子操作的实现方式放到对应的领域进行讲解。
实际上并发与分布式一直是很难区分的概念，在一个多线程/多进程环境中，我们也可以认为是一个分布式环境。而对于拥有多个物理或者虚拟机的系统本身也是并发的。

并发编程中的数据一致性/同步问题的解决往往是通过内存屏障、访问共享内存的方式解决，而分布式系统中往往是基于消息的方式进行解决。在整个并发编程中，我们需要着眼于并发编程模型、并发 IO 模型、同步以及高并发策略。一般来说，如果实现了多处理单元的同步，即串行化的按照某个先后顺序执行，是肯定可以保证数据一致性的。但是要保证数据一致性，譬如弱一致性或者最终一致性，并不一定要实现处理单元的同步。因此，同步与数据一致性的关注点以及所解决的问题还是有很大的差异的。同步即是希望能够以一定执行次序对于原本并发的处理单元进行重新编排，譬如常见的数据库中事务，也是同步的一个应用。而数据一致性更多着眼于不同的处理单元获取到一致的数据，譬如在分布式数据库中主子数据库之间的实时复制等，因此数据一致性的保证笔者是放到了分布式系统中进行讲解。而同步中主要会讲解锁与不同的锁策略的使用，以及事务的使用。很多地方也会称事务，譬如数据库事务是保证数据一致性的手段，此言甚对，不过事务本身还是通过类似于加锁的方式来将并发的操作串行化，从而达成原子操作这样一种目标。因此笔者还是把事务放在了并发编程中的同步这一归档下进行讲解。并发编程领域关心的是基本操作单元往往指多核或者多个线程，而分布式系统关注的数据一致性指的是多个节点之间的数据一致性。并发编程领域往往共享一个内存主存储器，而分布式系统中往往没有一个主存储器。

在并发编程中，笔者按照自己的理解将问题归纳为以下五个方面:

## 并行架构

人们通常认为并行等同于多核，但现代计算机在不同层次上都使用了并行技术。比如说，单   核的运行速度现今仍能每年不断提升的原因是：单核包含的晶体管数量，如同摩尔定律预测的那   样变得越来越多，而单核在位级和指令级两个层次上都能够并行地使用这些晶体管资源。

**位级(bit-level)并行**

为什么 32 位计算机比 8 位计算机运行速度更快？因为并行。对于两个 32 位数的加法，8 位计算

机必须进行多次 8 位计算，而 32 位计算机可以一步完成，即并行地处理 32 位数的 4 字节。

计算机的发展经历了 8 位、16 位、32 位，现在正处于 64 位时代。然而由位升级带来的性能改

善是存在瓶颈的，这也正是短期内我们无法步入 128 位时代的原因。

**指令级(instruction-level)并行**

现代 CPU 的并行度很高，其中使用的技术包括流水线、乱序执行和猜测执行等。

程序员通常可以不关心处理器内部并行的细节，因为尽管处理器内部的并行度很高，但是经   过精心设计，从外部看上去所有处理都像是串行的。

而这种“看上去像串行”的设计逐渐变得不适用。处理器的设计者们为单核提升速度变得越   来越困难。进入多核时代，我们必须面对的情况是：无论是表面上还是实质上，指令都不再串行   执行了。我们将在 2.2 节的“内存可见性”部分展开讨论。

**数据级(data)并行   数据级并行**

(也称为“单指令多数据”，SIMD)架构，可以并行地在大量数据上施加同一操   作。这并不适合解决所有问题，但在适合的场景却可以大展身手。

图像处理就是一种适合进行数据级并行的场景。比如，为了增加图片亮度就需要增加每一个像   素的亮度。现代 GPU(图形处理器)也因图像处理的特点而演化成了极其强大的数据并行处理器。

**任务级(task-level)并行**

终于来到了大家所默认的并行形式——多处理器。从程序员的角度来看，多处理器架构最明   显的分类特征是其内存模型(共享内存模型或分布式内存模型)。

对于共享内存的多处理器系统，每个处理器都能访问整个内存，处理器之间的通信主要通过   内存进行

![01g61INoQHaW](http://ifeve.com/wp-content/uploads/2015/03/01g61INoQHaW.jpg)

对于分布式内存的多处理器系统，每个处理器都有自己的内存，处理器之间的通信主要通过   网络进行，如图 1-2 所示。

对于分布式内存的多处理器系统，每个处理器都有自己的内存，处理器之间的通信主要通过   网络进行，如图 1-2 所示。

![QQ截图20150313001539](http://ifeve.com/wp-content/uploads/2015/03/QQ%E6%88%AA%E5%9B%BE20150313001539.png)

图 1-2  分布式内存的多处理器系统

通过内存通信比通过网络通信更简单更快速，所以用共享内存编程往往更容易。然而，当处   理器个数逐渐增多，共享内存就会遭遇性能瓶颈——此时不得不转向分布式内存。如果要开发一   个容错系统，就要使用多台计算机以规避硬件故障对系统的影响，此时也必须借助于分布式内存。

# Concurrent Primitive

# 进程:Process> - [Operating System - Processes](http://www.tutorialspoint.com/operating_system/os_processes.htm)

进程是程序动态的概念，它**用来表示程序在执行的一组数据结构**。这组数据结构中记录了指令加载到内存中的地址，打开的文件，线程信息，共享内存等。
每个进程可以有多个线程。它也是一组数据结构包括：下一条要执行的指令，寄存器，堆栈，状态等。一幅图来表示

![img](http://mmbiz.qpic.cn/mmbiz/nfxUjuI2HXgfr1c9ayFAp5Q1cB1jxxXbVcCEIyP3YSiboIdDeuicvslk3Ywce3pqkIicUQPQMG8KwU6lUJkyOPFTg/640?wx_fmt=png&wxfrom=5&wx_lazy=1) 如果程序没有启动任何线程，其实也会用到线程——主线程(图中的线程 1)。所以最后消耗 CPU 的线程而不是进程

## Process VS Thread:进程与线程的对比从概念上来看，Processes 与 Threads 很类似，最大的区别则在于线程仅可以访问其孵化进程的内存空间，并且进程可以很方便地控制其创建的线程。换言之，同一进程中的线程运行在共享内存空间中，而进程则运行在独立的内存空间中。具体表现上来看，其区别在于:- 线程之间共享其创建进程的地址空间，而进程则有独立的地址空间。- 线程能够直接访问其所属进程的数据块，而进程只能获取其父进程数据块的副本。- 同一进程中的线程可以直接通信，而不同的进程之间必须通过进程间通信技术。- 线程可以有效控制所属进程的其他线程，而进程只可以控制其创建的子进程。- 对于主线程的改变，譬如优先权变更等等会影响到其他线程，而对于父进程的变更不会影响到其子进程。- 从 Context Switch，上下文切换的角度来看，线程切换时虚拟内存空间并不会发生改变，而进程切换时则会发生改变。不过无论是线程还是进程的上下文切换都需要操作系统内核的调控。`Per process items | Per thread items------------------------------|-----------------Address space | Program counterGlobal variables | RegistersOpen files | StackChild processes | StatePending alarms |Signals and signal handlers |Accounting information |`需要注意的时，一般来说线程之间可以更方便地进行内存共享，但是进程也是可以共享内存与数据空间的，在进程的通信模型中就有一种方式是基于共享内存进行通信。

### Processes & Threads in Linux:Linux 中进程与线程其实没多大区别其实在 Linux 中(我实在不知道 Windows，我猜应该差不多)进程和线程是同一个数据结构——task_struct，对于内核(kernel)来说并没有进程和线程的区别，只有进程——kernel 称之为 task。所以在 Linux 中进程和线程并没有父子关系而是平行的结构，表示进程的数据结构填充的数据多一些，包括了打开文件，共享内存之类的，这个被称为主线程；其他线程的数据结构这些项目则为空，并且有一个“父进程”的指针，指向了“主线程”。明白了这一点我们就清楚了，操作系统调度的最小对象其实是——线程，但是名字叫 task，教科书上叫进程。。。。有点混乱了吗？所以我们引入一个新的术语——并发实体。所有 CPU 调度的最小单位我们统称为并发实体，无论是进程还是线程或者是其他的什么“怪胎”。(没错，我会在下一次介绍这些怪胎。)

this is a very general question, so I'll give a general answer.Java switched from green threads, to native threads early in its development. This does not mean that threads created on Windows and Linux will behave differently as both platforms will utilize native threads in their respective JVM implementations.The thread interface exposed to Java by each OS, and similarly the native interfaces to threading via pthreads and Windows threads are very similar.The biggest differences with respect to threading on the two platforms are that all threads on Linux are a form of process. Windows treats threads and processes very differently.In my personal experience, native threads on Windows are slightly more lightweight and may perform slightly better within single process applications. Correspondingly (and perhaps irrelevant), are that Windows processes are extremely heavyweight compared with their Linux counterparts.

227down voteLinux uses a 1-1 threading model, with (to the kernel) no distinction between processes and threads -- everything is simply a runnable task. \*
On Linux, the system call clone clones a task, with a configurable level of sharing, among which are:
CLONE_FILES: share the same file descriptor table (instead of creating a copy)CLONE_PARENT: don't set up a parent-child relationship between the new task and the old (otherwise, child's getppid() = parent's getpid())CLONE_VM: share the same memory space (instead of creating a COW copy)fork() calls clone(least sharing) and pthread_create() calls clone(most sharing). \*\*
forking costs a tiny bit more than pthread_createing because of copying tables and creating COW mappings for memory, but the Linux kernel developers have tried (and succeeded) at minimizing those costs.
Switching between tasks, if they share the same memory space and various tables, will be a tiny bit cheaper than if they aren't shared, because the data may already be loaded in cache. However, switching tasks is still very fast even if nothing is shared -- this is something else that Linux kernel developers try to ensure (and succeed at ensuring).
In fact, if you are on a multi-processor system, not sharing may actually be beneficial to performance: if each task is running on a different processor, synchronizing shared memory is expensive.

- Simplified.  CLONE_THREAD causes signals delivery to be shared (which needs CLONE_SIGHAND, which shares the signal handler table).
  \*\* Simplified. There exist both SYS_fork and SYS_clone syscalls, but in the kernel, the sys_fork and sys_clone are both very thin wrappers around the same do_fork function, which itself is a thin wrapper around copy_process. Yes, the terms process, thread, and task are used rather interchangeably in the Linux kernel...

The Linux kernel supports threads as first-class citizens. In fact to the kernel a thread isn't much different to a process, except that it shares a address space with another thread/process.Some old versions of ps even showed a separate process for each thread by default and newer versions can enable this behavior using the -m flag.

## 上下文切换

# 线程:Thread> - [聊聊线程技术与线程实现模型  ](http://my.oschina.net/andylucc/blog/689290)

**并发要解决的问题就是提高 CPU 的利用率**。明白这一点我们也就清楚了，并发只对非 CPU 密集型程序管用，如果 CPU 利用率非常高，更多的并发只会让情况更加糟糕
Linux (and indeed Unix) gives you a third option.Option 1 - processesCreate a standalone executable which handles some part (or all parts) of your application, and invoke it separately for each process, e.g. the program runs copies of itself to delegate tasks to.Option 2 - threadsCreate a standalone executable which starts up with a single thread and create additional threads to do some tasksOption 3 - forkOnly available under Linux/Unix, this is a bit different. A forked process really is its own process with its own address space - there is nothing that the child can do (normally) to affect its parent's or siblings address space (unlike a thread) - so you get added robustness.However, the memory pages are not copied, they are copy-on-write, so less memory is usually used than you might imagine.Consider a web server program which consists of two steps:Read configuration and runtime dataServe page requestsIf you used threads, step 1 would be done once, and step 2 done in multiple threads. If you used "traditional" processes, steps 1 and 2 would need to be repeated for each process, and the memory to store the configuration and runtime data duplicated. If you used fork(), then you can do step 1 once, and then fork(), leaving the runtime data and configuration in memory, untouched, not copied.
那么并发为什么一定是多线程而不是多进程呢？其实在 Linux 下进程和线程的创建成本没有什么区别(都是 task_struct)，但是进程之间可以共享数据的方式只能通过非常复杂的 IPC 来实现，**线程之间代码都是共享的，地址空间也是共享的，所以共享数据的方式更加高效。**(进程要考虑隔离，一个进程没有办法直接访问另一个进程；线程不用隔离，线程之间共享内存)
我们修改成多线程版本的 Web 服务器
`while (true){ request = next_http_request() request_work_in_thread(request)}`
request_work_in_thread 方法会启动一个线程(work 线程)，然后 CPU 开始执行 next_http_request 获得下一个请求。
request 对象是在主线程创建的，可以直接传递给 request_work_in_thread 中的 work 线程使用。
**我们提高 CPU 利用率所以需要并行，我们要提高并发实体之间共享数据的效率所以选择了线程作为并发实体的实现**

## 线程状态在正式学习 Thread 类中的具体方法之前，我们先来了解一下线程有哪些状态，这个将会有助于后面对 Thread 类中的方法的理解。

- 创建(new)状态: 准备好了一个多线程的对象- 就绪(runnable)状态: 调用了`start()`方法, 等待 CPU 进行调度- 运行(running)状态: 执行`run()`方法- 阻塞(blocked)状态: 暂时停止执行, 可能将资源交给其它线程使用- 终止(dead)状态: 线程销毁
  当需要新起一个线程来执行某个子任务时，就创建了一个线程。但是线程创建之后，不会立即进入就绪状态，因为线程的运行需要一些条件(比如内存资源，在前面的 JVM 内存区域划分一篇博文中知道程序计数器、Java 栈、本地方法栈都是线程私有的，所以需要为线程分配一定的内存空间)，只有线程运行需要的所有条件满足了，才进入就绪状态。
  当线程进入就绪状态后，不代表立刻就能获取 CPU 执行时间，也许此时 CPU 正在执行其他的事情，因此它要等待。当得到 CPU 执行时间之后，线程便真正进入运行状态。
  线程在运行状态过程中，可能有多个原因导致当前线程不继续运行下去，比如用户主动让线程睡眠(睡眠一定的时间之后再重新执行)、用户主动让线程等待，或者被同步块给阻塞，此时就对应着多个状态：time waiting(睡眠或等待一定的事件)、waiting(等待被唤醒)、blocked(阻塞)。
  当由于突然中断或者子任务执行完毕，线程就会被消亡。
  下面这副图描述了线程从创建到消亡之间的状态：
  下面这副图描述了线程从创建到消亡之间的状态：[!\](http://7xqch5.com1.z0.glb.clouddn.com/thread1_2.jpg)](http://7xqch5.com1.z0.glb.clouddn.com/thread1_2.jpg)
  在有些教程上将 blocked、waiting、time waiting 统称为阻塞状态，这个也是可以的，只不过这里我想将线程的状态和 Java 中的方法调用联系起来，所以将 waiting 和 time waiting 两个状态分离出来。
  注:sleep 和 wait 的区别:
  > - `sleep`是`Thread`类的方法,`wait`是`Object`类中定义的方法.> - `Thread.sleep`不会导致锁行为的改变, 如果当前线程是拥有锁的, 那么`Thread.sleep`不会让线程释放锁.> - `Thread.sleep`和`Object.wait`都会暂停当前的线程. OS 会将执行时间分配给其它线程. 区别是, 调用`wait`后, 需要别的线程执行`notify/notifyAll`才能够重新获得 CPU 执行时间.## 上下文切换对于单核 CPU 来说(对于多核 CPU，此处就理解为一个核)，CPU 在一个时刻只能运行一个线程，当在运行一个线程的过程中转去运行另外一个线程，这个叫做线程上下文切换(对于进程也是类似)。由于可能当前线程的任务并没有执行完毕，所以在切换时需要保存线程的运行状态，以便下次重新切换回来时能够继续切换之前的状态运行。举个简单的例
  >   子：比如一个线程 A 正在读取一个文件的内容，正读到文件的一半，此时需要暂停线程 A，转去执行线程 B，当再次切换回来执行线程 A 的时候，我们不希望线程 A
  >   又从文件的开头来读取。因此需要记录线程 A 的运行状态，那么会记录哪些数据呢？因为下次恢复时需要知道在这之前当前线程已经执行到哪条指令了，所以需要记录程序计数器的
  >   值，另外比如说线程正在进行某个计算的时候被挂起了，那么下次继续执行的时候需要知道之前挂起时变量的值时多少，因此需要记录 CPU 寄存器的状态。所以一
  >   般来说，线程上下文切换过程中会记录程序计数器、CPU 寄存器状态等数据。说简单点的：对于线程的上下文切换实际上就是 存储和恢复 CPU 状态的过程，它使得线程执行能够从中断点恢复执行。虽然多线程可以使得任务执行的效率得到提升，但是由于在线程切换时同样会带来一定的开销代价，并且多个线程会导致系统资源占用的增加，所以在进行多线程编程时要注意这些因素。以 Java 为例，多线程中上下文切换的影响在于 Every time we deliberately change a thread's status or attributes (e.g. by sleeping, waiting on an object, changing the thread's priority etc), we will cause a context switch. But usually we don't do those things so many times in a second to matter. Typically, the cause of excessive context switching comes from contention on shared resources, particularly synchronizedlocks:rarely, a single object very frequently synchronized on could become a bottleneck;more frequently, a complex application has several different objects that are each synchronized on with moderate frequency, but overall, threads find it difficult to make progress because theykeep hitting different contended locks at regular intervals.The second case is generally worse, because the juggling threads, each time they make a tiny bit of progress, fight for shared CPU cache, thus making each other less efficient each time they're switched in.

## Thread in Programming Language:编程语言中的多线程实现 Processes: Processes are OS-managed, and each is allocated its own address space (or context).Threads: Threads are OS-managed but share an address space (or context) with other threads running in the same process.Green Threads: Green Threads are user-space managed (not OS-managed) implementations of the “thread” concept.Goroutines: Goroutines are user-space managed and are multiplexed onto a pool of OS threads managed by the language’s runtime.Java uses OS threads to perform parallel executions of work, while Go uses (you guessed it) goroutines. This means they are very similar when it comes to parallelization because both languages execute their units of work on OS threads. There are, however, drastic distinctions between their concurrency models.If concurrency is the design or composition of simultaneous work, then we also need to talk about synchronization. By synchronization, I mean: How do the units of work running concurrently in your system synchronize with each other to communicate about their work?Java: Objects are shared between units of work. When a unit of work accesses this piece of shared data, it must first obtain a lock on it using the entity’s intrinsic lock (or monitor lock.)Go: Channels are shared between units of work. A channel is essentially an (optionally buffered) FIFO pipe. A unit of work may read or write to a channel.Java has solved the problem of synchronizing between units of work by providing a mechanism to synchronize access to memory shared between the units of work. This is effective and allows for the use of many design patterns that developers are already used to from non-concurrent programming.Go has solved the problem of synchronizing between units of work by re-framing the problem: Communicate over shared channels, and synchronized access is not necessary.In [Effective Go\*](http://golang.org/doc/effective_go.html#sharing) this is concisely explained with: _“Do not communicate by sharing memory; instead, share memory by communicating.”_

而对于多核的利用上，因为无论是 Java 还是 Go 都是基于 OS Threads 进行的封装，Java will benefit from multiple cores, if the OS distribute threads over the available processors. JVM itself do not do anything special to get its threads scheduled evenly across multiple cores. A few things to keep in mind:

## 线程中的三个概念### 有序性：对应串行化/锁/临界资源管理对于可见性，Java 提供了 volatile 关键字来保证可见性。

当一个共享变量被 volatile 修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。
而普通的共享变量不能保证可见性，因为普通共享变量被修改之后，什么时候被写入主存是不确定的，当其他线程去读取时，此时内存中可能还是原来的旧值，因此无法保证可见性。
另外，通过 synchronized 和 Lock 也能够保证可见性，synchronized 和 Lock 能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。### 原子性：对应原子操作/事务在 Java 中，对基本数据类型的变量的读取和赋值操作是原子性操作，即这些操作是不可被中断的，要么执行，要么不执行。
上面一句话虽然看起来简单，但是理解起来并不是那么容易。看下面一个例子 i：
请分析以下哪些操作是原子性操作：

```x = 10;         //语句1y = x;         //语句2x++;           //语句3x = x + 1;     //语句4

```

咋一看，有些朋友可能会说上面的 4 个语句中的操作都是原子性操作。其实只有语句 1 是原子性操作，其他三个语句都不是原子性操作。
语句 1 是直接将数值 10 赋值给 x，也就是说线程执行这个语句的会直接将数值 10 写入到工作内存中。
语句 2 实际上包含 2 个操作，它先要去读取 x 的值，再将 x 的值写入工作内存，虽然读取 x 的值以及 将 x 的值写入工作内存 这 2 个操作都是原子性操作，但是合起来就不是原子性操作了。
同样的，x++和 x = x+1 包括 3 个操作：读取 x 的值，进行加 1 操作，写入新的值。
所以上面 4 个语句只有语句 1 的操作具备原子性。
也就是说，只有简单的读取、赋值(而且必须是将数字赋值给某个变量，变量之间的相互赋值不是原子操作)才是原子操作。
不过这里有一点需要注意：在 32 位平台下，对 64 位数据的读取和赋值是需要通过两个操作来完成的，不能保证其原子性。但是好像在最新的 JDK 中，JVM 已经保证对 64 位数据的读取和赋值也是原子性操作了。
从上面可以看出，Java 内存模型只保证了基本读取和赋值是原子性操作，如果要实现更大范围操作的原子性，可以通过 synchronized 和 Lock 来实现。由于 synchronized 和 Lock 能够保证任一时刻只有一个线程执行该代码块，那么自然就不存在原子性问题了，从而保证了原子性。### 可见性：对应数据一致性在 Java 内存模型中，允许编译器和处理器对指令进行重排序，但是重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。
在 Java 里面，可以通过 volatile 关键字来保证一定的“有序性”(具体原理在下一节讲述)。另外可以通过 synchronized 和 Lock 来保证有序性，很显然，synchronized 和 Lock 保证每个时刻是有一个线程执行同步代码，相当于是让线程顺序执行同步代码，自然就保证了有序性。
另外，Java 内存模型具备一些先天的“有序性”，即不需要通过任何手段就能够得到保证的有序性，这个通常也称为 happens-before 原则。如果两个操作的执行次序无法从 happens-before 原则推导出来，那么它们就不能保证它们的有序性，虚拟机可以随意地对它们进行重排序。
下面就来具体介绍下 happens-before 原则(先行发生原则)：

- 程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作- 锁定规则：一个 unLock 操作先行发生于后面对同一个锁额 lock 操作- volatile 变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作- 传递规则：如果操作 A 先行发生于操作 B，而操作 B 又先行发生于操作 C，则可以得出操作 A 先行发生于操作 C- 线程启动规则：Thread 对象的 start()方法先行发生于此线程的每个一个动作- 线程中断规则：对线程 interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生- 线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过 Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行- 对象终结规则：一个对象的初始化完成先行发生于他的 finalize()方法的开始
  这 8 条原则摘自《深入理解 Java 虚拟机》。
  这 8 条规则中，前 4 条规则是比较重要的，后 4 条规则都是显而易见的。
  下面我们来解释一下前 4 条规则：
  对于程序次序规则来说，我的理解就是一段程序代码的执行在单个线程中看起来是有序的。注意，虽然这条规则中提到“书写在前面的操作先行发生于书写在后面的操作”，这个应该是程序看起来执行的顺序是按照代码顺序执行的，因为虚拟机可能会对程序代码进行指令重排序。虽然进行重排序，但是最终执行的结果是与程序顺序执行的结果一致的，它只会对不存在数据依赖性的指令进行重排序。因此，在单个线程中，程序执行看起来是有序执行的，这一点要注意理解。事实上，这个规则是用来保证程序在单线程中执行结果的正确性，但无法保证程序在多线程中执行的正确性。
  第二条规则也比较容易理解，也就是说无论在单线程中还是多线程中，同一个锁如果出于被锁定的状态，那么必须先对锁进行了释放操作，后面才能继续进行 lock 操作。
  第三条规则是一条比较重要的规则，也是后文将要重点讲述的内容。直观地解释就是，如果一个线程先去写一个变量，然后一个线程去进行读取，那么写入操作肯定会先行发生于读操作。
  第四条规则实际上就是体现 happens-before 原则具备传递性。

# 协程:Coroutine> [coroutines-and-fibers-why-and-when](https://medium.com/software-development-2/coroutines-and-fibers-why-and-when-5798f08464fd#.l10wfvyai)

如
果是内核线程(就是 fork 出来的，pthread_create 在 2.4 后最终也用 fork，具体参看其实现)，那么可以调度到多 cpu，内核支持线程
的诱导因素之一就是可以利用多 cpu 资源进行并行计算；如果是用户线程，那么就不能在多 cpu 上并行计算了，用户库线程的弊端之一就是不能利用多 cpu 资
源；具体到调度，就是当内核发现本 cpu 没有任何可运行线程时，就会去别的忙 cpu 上拽几个下来，当然这是内核自发进行的多 cpu 调度，作为用户也可以自
觉地将线程邦定到具体 cpu，更加确定的利用多 cpu，当然什么都不是任其发展的，内核不能随便从哪个忙 cpu 上拽线程，还要看线程愿不愿意(参见
linux 内核函数 cpuset_cpus_allowed)，另外还要考虑 smt(在 intel 平台即超线程)的兄弟 cpu，还要考虑 numa，你总不
能让本地 cpu 的线程跑到遥远的 cpu 上运行吧

协程，最准确的名字应该叫线程的线程，在不同的领域中也有不同的叫法，譬如纤程(Fiber)、绿色线程(Green Thread)等等。操作系统产生一个进程，进程再产生若干个线程并行的处理逻辑，线程的切换由操作系统负责调度。传统语言 C++
Java 等线程其实与操作系统线程是 1:1 的关系，每个线程都有自己的 Stack，Java 在 64 位系统默认 Stack 大小是 1024KB，所以指望一个
进程开启上万个线程是不现实的。但是实际上我们也不会这么干，因为起这么多线程并不能充分的利用 CPU，大部分线程处于等待状态，CPU 也没有这么核让线
程使用。所以一般线程数目都是 CPU 的核数。传统的 J2EE 系统都是基于每个请求占用一个线程去完成完整的业务逻辑(包括事务)。所以系统的吞吐能力取决于每个线程的操作耗时。如果遇到很耗时
的 I/O 行为，则整个系统的吞吐立刻下降，比如 JDBC 是同步阻塞的，这也是为什么很多人都说数据库是瓶颈的原因。这里的耗时其实是让 CPU 一直在等待 I
/O 返回，说白了线程根本没有利用 CPU 去做运算，而是处于空转状态。暴殄天物啊。另外过多的线程，也会带来更多的 ContextSwitch 开销。Java 的 JDK 里有封装很好的 ThreadPool，可以用来管理大量的线程生命周期，但是本质上还是不能很好的解决线程数量的问题，以及线程空转占用 CPU 资源的问题。   先阶段行业里的比较流行的解决方案之一就是单线程加上异步回调。其代表派是 node.js 以及 Java 里的新秀 Vert.x。
他们的核心思想是一样的，遇到需要进行 I/O 操作的地方，就直接让出 CPU 资源，然后注册一个回调函数，其他逻辑则继续往下走，I/O 结束后带着结果向事
件队列里插入执行结果，然后由事件调度器调度回调函数，传入结果。这时候执行的地方可能就不是你原来的代码区块了，具体表现在代码层面上，你会发现你的局
部变量全部丢失，毕竟相关的栈已经被覆盖了，所以为了保存之前的栈上数据，你要么选择带着一起放入回调函数里，要么就不停的嵌套，从而引起反人类的
Callback hell。因此相关的 Promise，CompletableFuture 等技术都是为解决相关的问题而产生的。但是本质上还是不能解决业务逻辑的割裂。说
了这么多，终于可以提一下协程了，协程的本质上其实还是和上面的方法一样，只不过他的核心点在于调度那块由他来负责解决，遇到阻塞操作，立刻 yield
掉，并且记录当前栈上的数据，阻塞完后立刻再找一个线程恢复栈并把阻塞的结果放到这个线程上去跑，这样看上去好像跟写同步代码没有任何差别，这整个流程可
以称为 coroutine，而跑在由 coroutine 负责调度的线程称为 Fiber。比如 Golang 里的 go 关键字其实就是负责开启一个 Fiber，让 func 逻辑跑在上面。而这一切都是发生的用户态上，没有发生在内核态上，也就是说没有 ContextSwitch 上的开销。
协程的实现库中笔者较为常用的譬如 Go Routine、[node-fibers](https://github.com/laverdet/node-fibers)、[Java-Quasar](https://github.com/puniverse/quasar).

[Fiber:coroutine support for v8 and node.](https://github.com/laverdet/node-fibers)
NIO(非阻塞 IO)是一种 IO 编程模型，Golang 中的 IO 底层实现方式和 java NIO 模型一致，通俗点说就是都采用了 EPOLL。 你在使用 golang 读文件的时候，goroutine 会默默的挂起，只是你不知道，当读完毕了，goroutine 再次恢复，但你不用担心，goroutine 的挂起和恢复没有 java 线程那样可怕，你可以认为 goroutine 的挂起和恢复就是保存和恢复几个变量的值，其实也是这样的。

剩下的就是 goroutine 和 java 线程的区别了，goroutine 是用户态的线程切换，java 采用的是系统线程切换，用汇编语言描述是一个(java)调用 int 80 软中断,一个没有。 意味着 goroutine 更轻量级，可以同时相应成千上万的线程切换，java 你创造上千个线程就有些吃力了。

因为 java 线程不能创造过多的线程，如果同时处理上万上千的请求时候，就要考虑在几十个线程来处理上万上千的请求，这就出现了很多请求和线程不可能一一对应，所以通常做法是每个线程分别处理单个请求各个阶段。好比流水线，请求是要加工的商品，每个线程处理一道工序，这样做的好处是每人都做自己熟悉的，对于程序来说每个线程执行的代码永远都是自己很短的一块，这样根据局部优化原理，更具备 CPU，内存亲和力，利于 JIT。说这样多，就是说如果线程和请求不能一一对应，流水线式的并发编程很麻烦，阅读性也很差，通常是线程 A 里面一段逻辑代码，线程 B 又有另一处处理的逻辑代码。

由于 goroutine 的轻便，你可以将请求和 goroutine 一一对应起来，不用考虑将请求在线程之间换来换去，只关心你的业务逻辑，这就是 goroutine 的好处。golang 的 goroutine 让你比 java 更容易编写并发程序，但性能不会有差别(目前来说，golang 性能还不能和 java 比，看过代码就知道了，GC 弱到爆)，代码不会减少，该写的逻辑还得写。ps，其实 golang 的(sched)go 程切换代码虽然原理和 java 的 fork-join 框架一样，但是 fork-join 比 golang 的 sched 代码牛逼不少，开始膜拜 Doug Lea 吧，golang 还有很长的路要走。   # Coroutine/Fiber/GreenThreadCoroutines [Conäì], and similarly fIbers and green threads, are a generalization of subroutines.While a subroutine is executed sequentially and straightly, a coroutine can suspend and resume its execution at distinct points in code. us, coroutines are good primitives for cooperative task handling, as coroutines are able to yield execution. We have identi.ed the advantages of cooperative task handling in chapter õ, especially in case of massive parallelism of asynchronous operations such as I/O.Coroutines are o.en used as low-level primitives, namely as an alternative to threads forimplementing high-level concurrency concepts. For example, actor-based systems and eventdriven platforms may use coroutines for their underlying implementation.ere are also several programming languages and language extensions that introduce coroutines or their variants to high-level programming languages. For instance, greenletÕ is a coroutine framework for Python, that is heavily used by high performance event loop frameworks such as geventó.Google Goì is a general-purpose programming language from Google that supports garbagecollection and synchronous message passing for concurrency (see below). Go targets usage scenarios similar to C/C++ by tendency. It does not supply threads for concurrent .ows of executions, but a primitive called goroutine, which is derived from coroutines. Goroutines are functions that are executed in parallel with their caller and other running goroutines.e runtime system maps goroutines to a number of underlying threads, which might lead to truely parallel execution. In other circumstances, multiple goroutines might also be executed by a single thread using cooperative scheduling. Hence, they are more powerful than conventional coroutines, as they imply parallel execution and communicate via synchronous message passing, and not just by yielding.

The heart of the problem lies in the question of mapping mm light-weight threads to nn physical processor cores (m≫nm≫n) given that the operating system has no prior knowledge on what is going on at the application level. As an inevitable consequence of this problem, developers come up with the idea of providing application level constructs to hint the underlying process scheduling mechanism about the concurrency operating at the application level. Among many other attempts, [coroutines](http://en.wikipedia.org/wiki/Coroutine) and [fibers](<http://en.wikipedia.org/wiki/Fiber_(computer_science)>) are the two well-known examples emanated from this research. Quoting from [Wikipedia](<http://en.wikipedia.org/wiki/Fiber_(computer_science)>),

> Fibers describe essentially the same concept as coroutines. The distinction, if there is any, is that coroutines are a language-level construct, a form of control flow, while fibers are a systems-level construct, viewed as threads that happen not to run concurrently. Priority is contentious; fibers may be viewed as an implementation of coroutines, or as a substrate on which to implement coroutines.
> If I would try to restate the given description from a more naive perspective targeting the Java Virtual Machine, _coroutines are language-level constructs to form a way of cooperative execution of multiple tasks, whereas fibers are light-weight threads scheduled preemptively by the virtual machine_. There is [a set of coroutine implementations](http://stackoverflow.com/questions/2846428/available-coroutine-libraries-in-java) available for JVM, but in this post I will try to stick with two particular fiber implementations: [Akka](http://akka.io/) from [Typesafe](http://typesafe.com/) and [Quasar](http://docs.paralleluniverse.co/quasar/) from [Parallel Universe](http://paralleluniverse.co/).

# Threading Models:线程模型> - [微服务的各种线程模型及其权衡](http://www.infoq.com/cn/articles/engstrand-microservice-threading)
