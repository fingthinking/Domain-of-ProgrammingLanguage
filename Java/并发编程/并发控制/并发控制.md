

# 并发策略
最常见的高并发系统，应该算是秒杀系统了。不过秒杀系统本身也会加上很多的前端控制，本章主要着眼于业务模块或者后台模块高并发下的应对策略。一般来说，高并发系统中存在的问题可以总结为：
- 共享资源的并发访问
- 计算型密集任务
- 单一热点资源的峰值流量

> - [高并发系统中的常见问题](http://weibo.com/p/1001603862417250608209)

## 共享资源的并发访问


乐观锁的核心算法是CAS(Compareand Swap，比较并交换)，它涉及到三个操作数：内存值、预期值、新值。当且仅当预期值和内存值相等时才将内存值修改为新值。这样处理的逻辑是，首先检查某 块内存的值是否跟之前我读取时的一样，如不一样则表示期间此内存值已经被别的线程更改过，舍弃本次操作，否则说明期间没有其他线程对此内存值操作，可以把 新值设置给此块内存。如图2-5-4-1，有两个线程可能会差不多同时对某内存操作，线程二先读取某内存值作为预期值，执行到某处时线程二决定将新值设置 到内存块中，如果线程一在此期间修改了内存块，则通过CAS即可以检测出来，假如检测没问题则线程二将新值赋予内存块。
![](http://img.blog.csdn.net/20140909202846012?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2FuZ3lhbmd6aGl6aG91/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
假如你足够细心你可能会发现一个疑问，比较和交换，从字面上就有两个操作了，更别说实际CAS可能会有更多的执行指令，他们是原子性的吗？如果非原 子性又怎么保证CAS操作期间出现并发带来的问题？我是不是需要用上节提到的互斥锁来保证他的原子性操作？CAS肯定是具有原子性的，不然就谈不上在并发 中使用了，但这个原子性是由CPU硬件指令实现保证的，即使用JNI调用native方法调用由C++编写的硬件级别指令，jdk中提供了Unsafe类 执行这些操作。另外，你可能想着CAS是通过互斥锁来实现原子性的，这样确实能实现，但用这种方式来保证原子性显示毫无意义。下面一个伪代码加深对CAS 的理解：
```
public class AtomicInt {
   private volatile int value;
   public final int get() {
       return value;
    }
public final int getAndIncrement() {
       for (;;) {
           int current = get();
           int next = current + 1;
           if (compareAndSet(current, next))
                return current;
       }
    }
   
   public final boolean compareAndSet(int expect, int update) {
     Unsafe类提供的硬件级别的compareAndSwapInt方法;
    }
}
```
现在已经了解乐观锁及CAS相关机制，乐观锁避免了悲观锁独占对象的现象，同时也提高了并发性能，但它也有缺点：

乐观锁是对悲观锁的改进，虽然它也有缺点，但它确实已经成为提高并发性能的主要手段，而且jdk中的并发包也大量使用基于CAS的乐观锁。
## 计算型密集任务缓存访问
## 单一热点资源峰值流量



# Introduction

介绍同步之前，首先我们来看下一个基于12306的典型的例子:
```

int total = get_total_from_tickets();
if ( total < 1) {
    not_found("No tickets!";);
    return FAILURE;
}
/*以上为步骤一*/

ticket tk = get_ticket_from_tickets();
--total;
/*以上为步骤二*/

split_ticket2user(tk);
/*步骤三*/
```
我们假设一次执行单元为其中一个步骤，但是这样的系统是不能使用的，因为它只能支持同时一个用户抢票。想象一下多个用户同时抢票的简单情形：系统中只剩一 张票了，用户A先开始抢票，整个过程是这样的：A先看还有一张票，也就是步骤一。此时，用户B使用了抢票插件，如有神助的在A完成步骤一尚未进行步骤二的 时候完成了步骤一，这个时候无论A和B谁先执行步骤二都意味只有一个人能抢到票。
导致上述事例结果的根本原因在于数据没有及时更新，即数据同步不及时。多线程下的具体情形可能是一个运行中的线程随时可能在它正在使用临界区(放置临界资 源的区域)的时候被抢占，而新调度的线程紧接着进入该临界区，这个时候就会发生竞争。如果是在对称处理器多线程下，就算一个线程能完整的完成它的任务而不 被抢断，但多处理器真正并行使得多条线程可以同时修改临界区，这使得及时同步数据变得非常困难。采用同步可以确保每条线程看到的数据是一致的。尤其是一条 线程在修改某一个值而其他线程也需要读取或修改这个值的时候，使用同步就可以保证访问的数据有效。
总结来说，要解决同步问题，主要是两种方式，一是加锁，而是原子操作。

## Terminology
### Volatile
原意是易变的。在计算机领域意思相同，指由该关键字修饰的变量的值易变，因此具有可见性。可见性是指当一个线程对临界资源进行修改后，在其他线程中可以看 到发生的变化。为了实现这种可见性，处理器和编译器忽略对该关键字修饰变量的优化，也就是不对它进行重排，也不会缓存。但该变量的使用却是非常危险的，因 为它的行为总是违反我们的直觉。具体原因有以下几方面：
虽然编译器不去对它进行优化，并且阻止volatile变量之间的重排(如C/C++和Java)。但是，它们可能和非volatile变量一起被重排 序。在C/C++和早期Java内存模型中确实是这么实现的。Java在新的内存模型下，不仅volatile变量不能彼此重排序，而且volatile 周围的普通字段的也不再能够随便的重排序了(和旧模型不同)，但是C/C++的volatile却并未支持。还有一点容易让人误解的是它并不具有原子性这 也是最容易犯错的地方。最后一点也是最能让使用者谨慎使用的理由是某些编译器或者是陈旧的Java虚拟机对该关键字的支持不完整。也许使用 volatile最安全的方式是严格限制到只是一个boolean值并且是在完全与周围变量或操作无关的场合。但不能放弃使用volatile的原因是在 如今的处理器架构下，内存模型强大到处理器对volatile的操作性能和非volatile相差无几。
### 自旋锁
Linux内核中最常见的锁，作用是在多核处理器间同步数据。这里的自旋是忙等待的意思。如果一个线程(这里指的是内核线程)已经持有了一个自旋锁，而另 一条线程也想要获取该锁，它就不停地循环等待，或者叫做自旋等待直到锁可用。可以想象这种锁不能被某个线程长时间持有，这会导致其他线程一直自旋，消耗处 理器。所以，自旋锁使用范围很窄，只允许短期内加锁。其实还有一种方式就是让等待线程睡眠直到锁可用，这样就可以消除忙等待。很明显后者优于前者的实现， 但是却不适用于此。我们来详细分析。如果我们使用第二种方式，我们要做几步操作：把该等待线程换出、等到锁可用在换入，有两次上下文切换的代价。这个代价 和短时间内自旋(实现起来也简单)相比，后者更能适应实际情况的需要。还有一点需要注意，试图获取一个已经持有自旋锁的线程再去获取这个自旋锁或导致死 锁，但其他操作系统并非如此。
自旋锁与互斥锁有点类似，只是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是 否该自旋锁的保持者已经释放了锁，"自旋"一词就是因此而得名。其作用是为了解决某项资源的互斥使用。因为自旋锁不会引起调用者睡眠，所以自旋锁的效率远 高于互斥锁。虽然它的效率比互斥锁高，但是它也有些不足之处：
    1、自旋锁一直占用CPU，他在未获得锁的情况下，一直运行－－自旋，所以占用着CPU，如果不能在很短的时 间内获得锁，这无疑会使CPU效率降低。
    2、在用自旋锁时有可能造成死锁，当递归调用时有可能造成死锁，调用有些其他函数也可能造成死锁，如 copy_to_user()、copy_from_user()、kmalloc()等。

自旋锁比较适用于锁使用者保持锁时间比较短的情况。正是由于自旋锁使用者一般保持锁时间非常短，因此选择自旋而不是睡眠是非常必要的，自旋锁的效率远高于互斥锁。信号量和读写信号量适合于保持时间较长的情况，它们会导致调用者睡眠，因此只能在进程上下文使用，而自旋锁适合于保持时间非常短的情况，它可以在任何上下文使用。如果被保护的共享资源只在进程上下文访问，使用信号量保护该共享资源非常合适，如果对共享资源的访问时间非常短，自旋锁也可以。但是如果被保护的共享资源需要在中断上下文访问(包括底半部即中断处理句柄和顶半部即软中断)，就必须使用自旋锁。自旋锁保持期间是抢占失效的，而信号量和读写信号量保持期间是可以被抢占的。自旋锁只有在内核可抢占或SMP(多处理器)的情况下才真正需要，在单CPU且不可抢占的内核下，自旋锁的所有操作都是空操作。另外格外注意一点：自旋锁不能递归使用。
### 互斥锁
即对互斥量进行分加锁，和自旋锁类似，唯一不同的是竞争不到锁的线程会回去睡会觉，等到锁可用再来竞争，第一个切入的线程加锁后，其他竞争失败者继续回去 睡觉直到再次接到通知、竞争。互斥锁算是目前并发系统中最常用的一种锁，POSIX、C++11、Java等均支持。处理POSIX的加锁比较普通 外，C++和Java的加锁方式很有意思。C++中可以使用一种AutoLock(常见于chromium等开源项目中)工作方式类似auto_ptr智 能指针，在C++11中官方将其标准化为std::lock_guard和std::unique_lock。Java中使用synchronized紧 跟同步代码块(也可修饰方法)的方式同步代码，非常灵活。这两种实现都巧妙的利用各自语言特性实现了非常优雅的加锁方式。当然除此之外他们也支持传统的类 似于POSIX的加锁模式。
### 读写锁
支持两种模式的锁，当采用写模式上锁时与互斥锁相同，是独占模式。但读模式上锁可以被多个读线程读取。即写时使用互斥锁，读时采用共享锁，故又叫共享-独 占锁。一种常见的错误认为数据只有在写入时才需要锁，事实是即使是读操作也需要锁保护，如果不这么做的话，读写锁的读模式便毫无意义。
### 重入
也叫做锁递归，就是获取一个已经获取的锁。不支持线程获取它已经获取且尚未解锁的方式叫做不可递归或不支持重入。带重入特性的锁在重入时会判断是否同一个 线程，如果是，则使持锁计数器+1(0代表没有被线程获取，又或者是锁被释放)。C++11中同时支持两种锁，递归锁 std::recursive_mutex和非递归std::mutex。Java的两种互斥锁实现以及读写锁实现均支持重入。POSIX使用一种叫做重 入函数的方法保证函数的线程安全，锁粒度是调用而非线程。
## 死锁
线程在执行过程中等待锁释放，如果存在多个线程相互等待已经被加锁的资源，就会造成死锁。大多数语言的锁实现都支持重入的一个重要原因是一个函数体内加锁 的代码段中经常会调用其他函数，而其他函数内部同样加了相同的锁，在不支持重入的情况下，执行线程总是要获取自己尚未释放的锁。也就是说该条线程试图获取 一个自己已经获取而尚未释放的锁。死锁就此产生。还有最经典的哲学家就餐问题。
## 线程饥饿
互斥锁中提到获取不到锁的线程回去睡眠等待下一次竞争锁，如果下一次仍然得不到，就继续睡眠，这种持续得不到锁的情况我们称之为饥饿。一个很有意思的例子是关于小米手机饥饿营销的。将小米手机比作竞争资源，抢手机的用户就是线程，每次开抢都抢不到的用户就是线程饥饿。和饥饿相对的是公平，操作系统调度程序负责这种公平，使用分片或nice或执行比等方式避免得不到调度的线程活活饿死。Java默认采用非公平的互斥锁(synchronized是强制的，Lock是可选的。关于Java内置锁和Lock锁的公平性讨论参见：[Java中的ReentrantLock和synchronized两种锁定机制的对比](http://my.eoe.cn/niunaixiaoshu/archive/5227.html))，但是公平锁因为要防止饥饿需要根据线程调度策略做调整，所以性能会受到影响，而且一般情况下某条线程饿死的情况鲜有发生(因为调度本来就是不公平的)，因此默认都是非公平的。
## CAS
中译名比较交换。目前有一种特殊的并发方式叫做无锁并发，通过上文的说明大家应该马上清楚要使用CAS达到正确同步须由处理其提供支持。有一个叫做 Lock_Free的算法提出了一种方案：确保执行它的所有线程中至少有一个能够继续往下执行。实现这个算法的技术叫做比较并交换，即CAS。CAS使用 乐观技术来更新值，如果在另一条线程更新了这个值，CAS可以检测到这个错误。现在大多数处理器架构(包括IA32和Sparc)都支持比较并交换CAS 的原子操作，X86下对应的是 CMPXCHG 汇编指令。CAS 原语负责将某处内存地址的值(1 个字节)与一个期望值进行比较，如果相等，则将该内存地址处的值替换为新值，CAS 操作用一行C代码描述如下：
return *add == old_val ? (*add = new_val) ? false;
目前windows内置、Gcc内置、C++11定义头文件和Java通过java.util.concurrent.atomic包提供对CAS操作的支持。

# Memory Consistency Memory
> - [内存一致性模型](http://blog.chinaunix.net/uid-25909722-id-3016122.html)

# Lock(锁实现)
锁，和现实中的门把锁有相同也有不同。我们把线程看做人，屋子是临界区，里面放的是东西是临界资源。每次只允许一个人进入屋子，当某个人进入屋子后会把门 锁上，当另一个人想进入屋子的时候，只能在门口等待(这里等待的方式有两种：忙等待，即循环查看是否屋子的人出来了。还有一种是先睡一觉，等屋子里的人出 来叫醒自己)。直到屋子里的人解锁出来，这个时候在门口等待的人才可以进去。这里有一个关键点需要保证：锁必须是原子性操作实现，决不能中途打断，由处理 器原语支持。锁的意义在于将操作做为一个执行单元以一种原子方式执行而不被打断，多线程下也不会互相干扰。但是锁会影响性能，这是因为一个加锁的临界资源 在被访问前必须获取对应的锁，获取该锁的线程将以独占的方式访问临界区。如果此时有其他线程同时访问临界区，则会因为无法获取这个锁而阻塞，显然，在临界 区强行通过加锁使线程执行串行化是需要牺牲一定的性能的。
## Semaphore(信号量)
信号量又称为信号灯，它是用来协调不同进程间的数据对象的，而最主要的应用是共享内存方式的进程间通信。本质上，信号量是一个计数器，它用来记录对某个资源(如共享内存)的存取状况。一般说来，为了获得共享资源，进程需要执行下列操作: 
　　 (1) 测试控制该资源的信号量。 
　　 (2) 若此信号量的值为正，则允许进行使用该资源。进程将信号量减1。 
　　 (3) 若此信号量为0，则该资源目前不可用，进程进入睡眠状态，直至信号量值大于0，进程被唤醒，转入步骤(1)。 
　　 (4) 当进程不再使用一个信号量控制的资源时，信号量值加1。如果此时有进程正在睡眠等待此信号量，则唤醒此进程。 

信号量与普通整型变量的区别：
①信号量(semaphore)是非负整型变量，除了初始化之外，它只能通过两个标准原子操作：wait(semap) , signal(semap) ; 来进行访问；
②操作也被成为PV原语(P来源于Dutch proberen"测试"，V来源于Dutch verhogen"增加")，而普通整型变量则可以在任何语句块中被访问； 
信号量与互斥锁之间的区别：
1. 互斥量用于线程的互斥，信号线用于线程的同步。  

这是互斥量和信号量的根本区别，也就是互斥和同步之间的区别。  

互斥：是指某一资源同时只允许一个访问者对其进行访问，具有唯一性和排它性。但互斥无法限制访问者对资源的访问顺序，即访问是无序的。  

同步：是指在互斥的基础上(大多数情况)，通过其它机制实现访问者对资源的有序访问。在大多数情况下，同步已经实现了互斥，特别是所有写入资源的情况必定是互斥的。少数情况是指可以允许多个访问者同时访问资源  


Semaphore是多线程编程时一个很重要的概念，概念本身并不复杂，但要做到正确使用却不容易。这里我们从面向对象的角度来理解这个概念。现实生活中的任何对象或实体，我们都可以用class来描述它。Semaphore也不例外，如果用class定义应该是这样：

![](https://github.com/music4kid/music4kid.github.io/blob/master/images/class_semaphore.png?raw=true)

Semaphore对象里包含一个count值和一个队列对象，另外有两个对外public的方法，wait()和signal()，需要特别注意的事，count值代表的资源数量是不能为负的。为了理解这些属性和方法，我们可以类比一个现实生活中的例子。大家去餐厅吃饭，假设这个餐厅有10个座位，有20个吃货随机出发去这个餐厅吃饭，那么对应关系是这样的：

- count = 10，10个座位。
- queue，餐厅位置有限，为了避免混乱，餐厅肯定会吃货们排队。
- wait()，吃货到了餐厅找服务员要位置点餐，这个行为就是wait。
- signal()，吃货吃完了买单离开位置，这个行为就是signal。

这其实是一个信号量应用的典型场景，这里关键在于正确理解wait和signal发生时都有哪些细节步骤。用代码来描述大概是这样：

![](https://github.com/music4kid/music4kid.github.io/blob/master/images/semaphore_wait_signal.png?raw=true)

**wait**

具体到餐厅到例子，20个人随机出发去餐厅吃饭，有10个人先到，然后挨个执行wait。前10个人执行wait的时候是有位置的，所以count>0，这10个人每人都消耗掉一个座位开始吃饭。到第11个人到了都时候，count＝＝0，没有位置了，所以被suspend，开始加入排队都队列等待。后续所有人都慢慢的到来，但和第11个人一样，都只能排队。

**signal**

过了一段时间之后，有个人吃好结账离开了餐厅。这时候如果没有人在排队，位置数量count ＋＋，没有其它事情发生。但如果有人在排队，比如上面的情况，有10个人在等待位置，餐厅会把排在第一个的人安排到刚才空出来的位置，count值没有变化，但队列的人少了一个。

**特别注意**

对于wait和signal还有两点需要特别注意。也是平时我们使用semaphore时比较容易产生bug的地方。

- wait和signal都是原子操作。可以简单理解为上面代码里wait(),signal()两个函数都是加锁的。这个特性其实让semaphore的行为变得更简单清晰。大家想象，如果到餐厅的10个人是同时到达的，但不是依次询问餐厅是否有位置，而是10张嘴同时说话，同时找餐厅要位置，显然情况会变得复杂不好处理。
- wait或者signal调用的顺序是不确定的。上面的例子中每个人都是随机时间出发，到达餐厅的顺序也是随机的，并不一定先出发的就先到。同理每个人吃饭的时间长短也不一定，有人快有人慢，所以吃好离开餐厅的时间点也是随机的。这里每个人都代表一个线程，因为操作系统线程调度策略导致到底哪个线程先执行也是不确定的。

## Mutex(互斥量)

理解了Semaphore，再看Mutex就很简单了。可以把Mutex理解成count ＝＝ 1的Semaphore。在使用Mutex的场景下，永远都只允许有一个线程在占有资源，其它的线程都必须等待。建议大家按照count＝1把上面Semaphore的例子再在脑子里过一遍，加深理解。

## Lock(锁)

上面说的Semaphore和Mutex都是操作系统层面的基础概念。但具体到某个平台的时候，平台会对这两个概念再做一次封装以方便使用。比如在iOS上就有NSLock这个类，提供lock(),unlock()两个功能。但其实Lock在概念上和Mutex是一致的。当然平台既然做了封装就会提供额外的功能或者做一些额外的处理。这也是为什么在iOS里，NSLock的性能会比pthread_mutex会差一些。iOS里各种锁性能对比可以参考[这篇文章](http://perpendiculo.us/2009/09/synchronized-nslock-pthread-osspinlock-showdown-done-right/)。

## Condition(条件变量)

另一个遇到机会相对较少的概念是Condition，Condition理解起来很容易和上面几个概念混淆，但是只要和Semaphore对比下不同之处理解就很简单了。Condition甚至可以理解成一种“特殊”的Semaphore。特殊之处就在于它没有count(资源数)。它也有wait和signal两种行为。用代码简单表示是这样的：

![](https://github.com/music4kid/music4kid.github.io/blob/master/images/class_condition.png?raw=true)

逻辑其实变更简单了，可以从事件的角度去看待Condition。每次condition调用wait的时候，表示它想等待某个事件的发生(不管之前有没有发生过)，所以一定是加入到等待队列当中。调用signal的时候，表示这个事件发生了，如果有线程在队列里等待，则取出其中一个来执行，后面的继续等待后续事件。如果队列是空的，这个事件就丢了，当什么也没有发生过。所以这里的关键在于对count(资源)和事件的理解。

## 可重入锁
锁作为并发共享数据，保证一致性的工具，在JAVA平台有多种实现(如 synchronized 和 ReentrantLock等等 ) 。这些已经写好提供的锁为我们开发提供了便利，但是锁的具体性质以及类型却很少被提及。本系列文章将分析JAVA下常见的锁名称以及特性，为大家答疑解 惑。可重入锁，也叫做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受影响。
在JAVA环境下 ReentrantLock 和synchronized 都是 可重入锁。
```
public class Test implements Runnable{
 public synchronized void get(){
  System.out.println(Thread.currentThread().getId());
 //在子方法里又进入了锁
  set();
 }
 public synchronized void set(){
  System.out.println(Thread.currentThread().getId());
 }
 @Override
 public void run() {
  get();
 }
 public static void main(String[] args) {
  Test ss=new Test();
  new Thread(ss).start();
  new Thread(ss).start();
  new Thread(ss).start();
 }
}


```
两个例子最后的结果都是正确的，即 同一个线程id被连续输出两次。
结果如下：
```
Threadid: 8
Threadid: 8
Threadid: 10
Threadid: 10
Threadid: 9
Threadid: 9
```
可重入锁最大的作用是避免死锁。
我们以自旋锁作为例子。
```
public class SpinLock {
 private AtomicReference<Thread> owner =new AtomicReference<>();
 public void lock(){
  Thread current = Thread.currentThread();
  while(!owner.compareAndSet(null, current)){
  }
 }
 public void unlock (){
  Thread current = Thread.currentThread();
  owner.compareAndSet(current, null);
 }
}

```
对于自旋锁来说：
1、若有同一线程两调用lock() ，会导致第二次调用lock位置进行自旋，产生了死锁
说明这个锁并不是可重入的。(在lock函数内，应验证线程是否为已经获得锁的线程)
2、若1问题已经解决，当unlock()第一次调用时，就已经将锁释放了。实际上不应释放锁。
(采用计数次进行统计)
```
public class SpinLock1 {
 private AtomicReference<Thread> owner =new AtomicReference<>();
 private int count =0;
 public void lock(){
  Thread current = Thread.currentThread();
  if(current==owner.get()) {
   count++;
   return ;
  }
  while(!owner.compareAndSet(null, current)){
  }
 }
 public void unlock (){
  Thread current = Thread.currentThread();
  if(current==owner.get()){
   if(count!=0){
    count--;
   }else{
    owner.compareAndSet(current, null);
   }
  }
 }
}
```


# Lock Memory Model(锁的内存模型)
在多处理器下，多个处理器共享主存。为了效率并不要求处理器将更新立即同步到主存上。处理器拥有自己的缓存以保存这些更新，并且定期与主存同步。这种需要 定期同步的方式是为了保证缓存一致性(Cache Coherence)。在缓存一致性许允许的范围内，多个处理器可以拥有同一个共享数据的不同状态。内存模型提供了一种保证：规定共享数据在不同线程间的 状态总是一致的。它的复杂性在于要协调处理器和编译器在与多线程程序执行时的性能与数据同步状态之间的平衡。处理器和编译器的工作是通过优化指令执行顺序 添加缓存来加快指令执行速度。内存模型采用一组屏障指令来保证存储的一致性，当然是在尽可能少的牺牲性能的前提下。具体的编译器和处理器加快指令执行的方 法是代码重排、指令重排以及缓存。内存模型利用处理器提供的一组指令来保护数据一致性，这种方式称为内存屏障(Memory Barriers)。
## 代码重排
代码重排是指编译器对用户代码进行优化以提高代码的执行效率，优化前提是不改变代码的结果，即优化前后代码执行结果必须相同。譬如：
```
int a = 1, b = 2, c = 3;
void test() {
    a = b + 1;
    b = c + 1;
    c = a + b;
}
```
在gcc下的汇编代码test函数体代码如下：
编译参数: -O0
```

movl b(%rip), %eax
addl $1, %eax
movl %eax, a(%rip)
movl c(%rip), %eax
addl $1, %eax
movl %eax, b(%rip)
movl a(%rip), %edx
movl b(%rip), %eax
addl %edx, %eax
movl %eax, c(%rip)
```
编译参数：-O3
```

movl b(%rip), %eax                  ;将b读入eax寄存器
leal 1(%rax), %edx                  ;将b+1写入edx寄存器
movl c(%rip), %eax                  ;将c读入eax
movl %edx, a(%rip)                  ;将edx写入a
addl $1, %eax                       ;将eax+1
movl %eax, b(%rip)                  ;将eax写入b
addl %edx, %eax                     ;将eax+edx
movl %eax, c(%rip)                  ;将eax写入c
```
我在-O3的汇编下做了详细的注释，参照注释和原C代码理解这两段汇编代码应该不难。当然编译器优化并没有做多少工作，这是因为并未有多少无用代码。但是如果我们的test函数体内只写了100行的a++; 那汇编指令使用-O1就会优化这100行代码成一条：
```
addl $100, a(%rip)
```
然而，上面的代码更有意义来说明编译器优化并且其中将后面可能用到的汇编指令做了清楚的注释以说明其含意，便于下文对汇编代码的理解。如果觉得上述代码的指令重排难于理解或是不够充分，接下来看这段C代码和gcc -O3下的汇编代码：
```

int a = 1, b = 2;
void test1() {
    a = b + 1;
    b = 39;
}
------
movl b(%rip), %eax ;读取b给eax
movl $39, b(%rip) ;向b写入39
addl $1, %eax ;eax+1
movl %eax, a(%rip) ;将eax写回a
```
很显然b先被赋值，然后才是a。也就是说在该函数中，代码的执行顺序发生了变化，但却不影响最终结果。编译器重排是根据指令之间是否有数据依赖关系来决定 的，虽然看似C代码间存在依赖，但是重排却是指令级别的。顺便分析下这里为什么会把b的赋值操作提前进行呢？寄存器读入b的值时对b进行缓存，再写入的时 候直接从寄存器缓存中取出赋值避免了再次从高速缓存甚至主存中取出b再赋值。可以动手写实验代码验证，很容易发现确实编译器会将相关变量的操作提取到一起 执行，这是因为处理器充分利用寄存器缓存来加速指令执行。
## 指令重排
大多数主流的处理器为了效率可以调整读写操作的顺序，但为什么这么做呢？处理器在执行指令期间，会把指令按照自适应处理的最优情形进行重新排序，使指令执 行时间变得更短(绝大多数情形下，前提是不改变程序的执行结果)。处理器的具体做法是优化其指令流水线(Instruction pipeline)以减少指令执行时间。
我们假设在一条简单的流水线中，完成一个指令可能需要5层。对于一些并不互相依赖的指令，要在最佳性能下运算，当第一个指令被运行时，这个流水线需要运行 紧接着的4条独立的指令。如果有指令依赖前面已经执行的指令，那处理器就会通过某种方式延缓该条指令执行直到依赖的指令执行完毕。使用多个层执行指令，处 理器可以显著提高性能从而减少指令运行所需要的周期。处理器使用这种优化Pipeline的方式一方面提高了指令执行效率，但另一方面却出现了另一个麻 烦。单处理器下执行指令调整顺序在多线程并发的时候出现了困难。我们假设有两个处理器，每一个处理器执行一条线程，对于涉及到同一段代码对非局部变量赋值 的顺序会因为的每一个处理器对各自指令顺序调整而变得混乱。
看下启动服务器的示例代码：
全局：bool enable = false;
```

void start_server() {
    open_server();
    enable = true;
}

void http_server() {
    if(enable) handle_request();
}
```
我们使用两条线程在多处理器下并发，一条线程A负责启动服务器start_server(),另一条线程B负责处理http请求。由于上述所说的处理器会 将互不依赖的两条指令调换顺序(这里暂且忽略编译器优化)，我们有理由相信会出现这样的情形：A线程：enable = true;已经执行，但open_server();还未调用。此时另一条线程B执行http_server();发现服务可用，直接使用未打开的服务器 来处理请求。
同样的我们如果在此忽略处理器重排，只使用编译器的代码重排也会导致上述问题。问题的复杂性在于编译器和处理器同时作用下，指令的执行顺序更加神秘莫测。 但更糟糕的是还有一种为了效率缓存指令执行结果使数据不能及时更新的因素影响数据同步，这个因素就是CPU Cache。
## CPU Cache
A trip to main memory costs hundreds of clock cycles on commodity hardware. Processors use caching to decrease the costs of memory latency by orders of magnitude.
——Dennis Byrne
上面这句话的大体意思是说对内存的一次访问需要花费数百个时钟周期，处理器使用缓存减少内存延迟的成本。这就是引入缓存的原因。CPU内部结构根据物理距 离依次是：处理器、寄存器、CPU Cache。我根据自己机器上的CPU Cache的信息用cacoo画了一个简单版本的CPU内部结构图：
![](http://img.blog.csdn.net/20140128171814125?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYWppYW4wMDU=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

![](http://mmbiz.qpic.cn/mmbiz/nfxUjuI2HXh2dBjY4yO4xib5qK2cfEDZwIFicOL0VADibmMib3kZSCibkZ15ricFD3qXicJicwXOKibw7ecBjicq7kJpzCbA/640?wx_fmt=png&wxfrom=5&wx_lazy=1)

参照上图，由于既要满足存储容量需求又要满足CPU的高速存取需求，目前的缓存被设计为多级L1, L2 。。。。。。Ln。每一级的访问周期比上一级多一个数量级(并不绝对)。L1靠近处理器，用以满足高速存取的需求，Ln靠近主存，容积随之变大。目前主流 是三级缓存。Linux下查看CPU Cache可以进入/sys/devices/system/cpu/cpu0/cache，例如我的机器下查看高速缓存类型的结果：
```
lunatic@ubuntu:/sys/devices/system/cpu/cpu0/cache$ cat -n index0/type index1/type index2/type index3/type

1 Data //L1, Data Cache
2 Instruction //L1, instruction Cache
3 Unified //L2，代表不共享
4 Unified //L3，代表不共享
```
处理器缓存执行指令结果于自己的cache上，等满足一定条件如遇到请求刷新的指令，再将缓存结果一并输出。
用一段挂起服务器的代码就可以看出缓存对数据同步的影响：
全局：Bool enable = true;
```

void halt_server() {
    enable = false;
    close_server();
}
void http_server() {
    if(enable) handle_request();
}
```
还是双核双线程模拟执行的情形：线程A执行服务器挂起操作，但因为CPU缓存影响，服务器已经挂起，但并未同步更新到全局enable。而另一条线程B检测到enable可用时，就继续提供请求处理服务导致出现一个已经关闭的服务器还提供服务的情形。
对于处理器使用高速缓存以追求效率的解释可以通过对C语言标准库的IO缓冲分析来解释。C语言的IO输出默认有三种缓冲方式：无缓冲、行缓冲和块缓冲。我 们以printf(默认行缓冲)说明，printf函数的作用是将格式化后的字符逐个发送到输出缓冲区，缓冲区被刷新直到遇到换行符。这样做无疑能提高效 率，并且延迟了写输出。再来看输入scanf，从键盘缓冲区读取非空白字符数据，而把换行符’\n’遗留在缓冲区，引发的问题是如果同时用 getchar、gets等不会忽略’\n’的函数读取，就只能读取到无意义的’\n’。C语言IO系统的读入写出和处理器缓存非常类似。因此，解决C语 言缓冲区导致数据不一致的方案可以帮助理解处理器的缓存处理。C语言使用fflush函数或其他类似功能函数在必要的时候刷新或者清空缓冲区。处理器在缓 存处理上采取类似的做法，但远比其复杂的是处理器要同时兼顾指令缓存以及指令排序还有其他影响到数据同步的各方面软硬件因素，这种多方兼顾的方式就叫做内存屏障。
## 内存屏障
现在我们来谈下多处理器下的共享内存数据同步问题。多处理器同时访问共享主存，每个处理器都要对读写进行重新排序，一旦数据更新，就需要同步更新到主存上 (这里并不要求处理器缓存更新之后立刻更新主存)。在这种情况下，代码和指令重排，再加上缓存延迟指令结果输出导致共享变量被修改的顺序发生了变化，使得 程序的行为变得无法预测。为了解决这种不可预测的行为，处理器提供一组机器指令来确保指令的顺序要求，它告诉处理器在继续执行前提交所有尚未处理的载入和 存储指令。同样的也可以要求编译器不要对给定点以及周围指令序列进行重排。这些确保顺序的指令称为内存屏障。具体的确保措施在程序语言级别的体现就是内存 模型的定义。POSIX、C++、Java都有各自的共享内存模型，实现上并没有什么差异，只是在一些细节上稍有不同。这里所说的内存模型并非是指内存布 局，特指内存、Cache、CPU、写缓冲区、寄存器以及其他的硬件和编译器优化的交互时对读写指令操作提供保护手段以确保读写序。将这些繁杂因素可以笼 统的归纳为两个方面：重排和缓存，即上文所说的代码重排、指令重排和CPU Cache。简单的说内存屏障做了两件事情：**拒绝重排，更新缓存**。
C++11提供一组用户API std::memory_order来指导处理器读写顺序。Java使用happens-before规则来屏蔽具体细节保证，指导JVM在指令生成的过程中穿插屏障指令。
内存屏障也可以在编译期间指示对指令或者包括周围指令序列不进行优化，称之为编译器屏障，相当于轻量级内存屏障，它的工作同样重要，因为它在编译期指导编译器优化。屏障的实现稍微复杂一些，我们使用一组抽象的假想指令来描述内存屏障的工作原理。
使用MB_R、MB_W、MB来抽象处理器指令为宏：
MB_R代表读内存屏障，它保证读取操作不会重排到该指令调用之后。
MB_W代表写内存屏障，它保证写入操作不会重排到该指令调用之后。
MB代表读写内存屏障，可保证之前的指令不会重排到该指令调用之后。
这里用一个例子描述在多线程并发中使用内存屏障的意义：
全局变量：bool has = false, Ticket t = NULL;
```

Thread A                                                                Thread B
has = has_remain_ticket();/*有票*/             -------------------
mb();/*表示在该处插入MB指令 */               if(has) {;
--------------- --------------- --------------- ------  t = get_a_ticket();
--------------- --------------- --------------- ------  to_user(t);
--------------- --------------- --------------- -------------- }
```
用两条线程A、B执行，A中加入的内存屏障有票，再通知用户，不会发生先通知用户，再去看有票。B中的内存屏障保证必须取到票再交给用户，防止尚未取票，就交给用户。
这些屏障指令在单核处理器上同样有效，因为单处理器虽不涉及多处理器间数据同步问题，但指令重排和缓存仍然影响数据的正确同步。指令重排是非常底层的且实 现效果差异非常大，尤其是不同体系架构对内存屏障的支持程度，甚至在不支持指令重排的体系架构中根本不必使用屏障指令。具体如何使用这些屏障指令是支持的 平台、编译器或虚拟机要实现的，我们只需要使用这些实现的API(指的是各种并发关键字、锁、以及重入性等，下节详细介绍)。这里的目的只是为了帮助更好 的理解内存屏障的工作原理。
内存屏障的意义重大，是确保正确并发的关键。通过正确的设置内存屏障可以确保指令按照我们期望的顺序执行。这里需要注意的是内存屏蔽只应该作用于需要同步的指令或者还可以包含周围指令的片段。如果用来同步所有指令，目前绝大多数处理器架构的设计就会毫无意义。
# Lock Strategy(锁策略)








